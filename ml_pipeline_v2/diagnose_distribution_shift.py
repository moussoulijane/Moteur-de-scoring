"""
DIAGNOSTIC DE DISTRIBUTION SHIFT
Analyse les diff√©rences entre les donn√©es d'entra√Ænement (2024) et nouvelles donn√©es (ex: 2023)
pour identifier pourquoi le mod√®le pr√©dit des probabilit√©s anormales

Usage:
    python ml_pipeline_v2/diagnose_distribution_shift.py --reference_file data/raw/reclamations_2024.xlsx --new_file data/raw/reclamations_2023.xlsx
"""
import sys
sys.path.append('src')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from preprocessor_v2 import ProductionPreprocessorV2

# Configuration
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (20, 12)


class DistributionShiftDiagnostic:
    """Diagnostic de distribution shift entre donn√©es de r√©f√©rence et nouvelles donn√©es"""

    def __init__(self, reference_file, new_file):
        self.reference_file = reference_file
        self.new_file = new_file
        self.output_dir = Path('outputs/diagnostic_shift')
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Colonnes importantes
        self.numeric_cols = ['Montant demand√©', 'D√©lai estim√©', 'anciennete_annees']
        self.categorical_cols = ['Famille Produit', 'Cat√©gorie', 'Sous-cat√©gorie', 'Segment', 'March√©']

    def load_data(self):
        """Charger les donn√©es"""
        print("\n" + "="*80)
        print("üìÇ CHARGEMENT DES DONN√âES")
        print("="*80)

        self.df_ref = pd.read_excel(self.reference_file)
        self.df_new = pd.read_excel(self.new_file)

        print(f"‚úÖ Donn√©es de r√©f√©rence (train): {len(self.df_ref)} lignes")
        print(f"‚úÖ Nouvelles donn√©es (inf√©rence): {len(self.df_new)} lignes")

        # Nettoyer les colonnes num√©riques
        from preprocessor_v2 import ProductionPreprocessorV2
        preprocessor = ProductionPreprocessorV2()
        for df in [self.df_ref, self.df_new]:
            for col in self.numeric_cols:
                if col in df.columns:
                    df[col] = preprocessor._clean_numeric_column(df[col])

    def analyze_numeric_distributions(self):
        """Analyser les distributions des variables num√©riques"""
        print("\n" + "="*80)
        print("üìä ANALYSE DES DISTRIBUTIONS NUM√âRIQUES")
        print("="*80)

        fig, axes = plt.subplots(len(self.numeric_cols), 3, figsize=(20, 6*len(self.numeric_cols)))
        fig.suptitle('COMPARAISON DISTRIBUTIONS NUM√âRIQUES: R√©f√©rence vs Nouvelles Donn√©es',
                     fontsize=16, fontweight='bold', y=0.995)

        results = []

        for idx, col in enumerate(self.numeric_cols):
            if col not in self.df_ref.columns or col not in self.df_new.columns:
                continue

            # Donn√©es
            ref_data = self.df_ref[col][self.df_ref[col] > 0]
            new_data = self.df_new[col][self.df_new[col] > 0]

            # Statistiques
            ref_stats = {
                'mean': ref_data.mean(),
                'median': ref_data.median(),
                'std': ref_data.std(),
                'min': ref_data.min(),
                'max': ref_data.max(),
                'q25': ref_data.quantile(0.25),
                'q75': ref_data.quantile(0.75),
                'zeros': (self.df_ref[col] == 0).sum() / len(self.df_ref)
            }

            new_stats = {
                'mean': new_data.mean(),
                'median': new_data.median(),
                'std': new_data.std(),
                'min': new_data.min(),
                'max': new_data.max(),
                'q25': new_data.quantile(0.25),
                'q75': new_data.quantile(0.75),
                'zeros': (self.df_new[col] == 0).sum() / len(self.df_new)
            }

            # Calcul des √©carts
            mean_diff_pct = ((new_stats['mean'] - ref_stats['mean']) / ref_stats['mean'] * 100)
            median_diff_pct = ((new_stats['median'] - ref_stats['median']) / ref_stats['median'] * 100)

            results.append({
                'Variable': col,
                'Ref_Mean': ref_stats['mean'],
                'New_Mean': new_stats['mean'],
                'Mean_Diff_%': mean_diff_pct,
                'Ref_Median': ref_stats['median'],
                'New_Median': new_stats['median'],
                'Median_Diff_%': median_diff_pct,
                'Ref_Zeros_%': ref_stats['zeros'] * 100,
                'New_Zeros_%': new_stats['zeros'] * 100
            })

            print(f"\nüìä {col}:")
            print(f"   R√©f√©rence - Mean: {ref_stats['mean']:,.2f}, Median: {ref_stats['median']:,.2f}")
            print(f"   Nouvelles - Mean: {new_stats['mean']:,.2f}, Median: {new_stats['median']:,.2f}")
            print(f"   ‚ö†Ô∏è  Diff√©rence: Mean {mean_diff_pct:+.1f}%, Median {median_diff_pct:+.1f}%")
            if abs(mean_diff_pct) > 20 or abs(median_diff_pct) > 20:
                print(f"   üö® ALERTE: Diff√©rence > 20% !")

            # Plot 1: Histogrammes superpos√©s
            ax = axes[idx, 0]
            # Limiter aux percentiles pour meilleure visualisation
            ref_plot = ref_data[ref_data <= ref_data.quantile(0.95)]
            new_plot = new_data[new_data <= new_data.quantile(0.95)]

            ax.hist(ref_plot, bins=50, alpha=0.5, label='R√©f√©rence (train)', color='blue', density=True)
            ax.hist(new_plot, bins=50, alpha=0.5, label='Nouvelles donn√©es', color='red', density=True)
            ax.axvline(ref_stats['median'], color='blue', linestyle='--', linewidth=2, label=f'M√©diane Ref: {ref_stats["median"]:.0f}')
            ax.axvline(new_stats['median'], color='red', linestyle='--', linewidth=2, label=f'M√©diane New: {new_stats["median"]:.0f}')
            ax.set_xlabel(col, fontweight='bold')
            ax.set_ylabel('Densit√©', fontweight='bold')
            ax.set_title(f'Distribution: {col}', fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)

            # Plot 2: Box plots comparatifs
            ax = axes[idx, 1]
            box_data = [ref_plot, new_plot]
            bp = ax.boxplot(box_data, labels=['R√©f√©rence', 'Nouvelles'], patch_artist=True)
            bp['boxes'][0].set_facecolor('blue')
            bp['boxes'][1].set_facecolor('red')
            for box in bp['boxes']:
                box.set_alpha(0.6)
            ax.set_ylabel(col, fontweight='bold')
            ax.set_title(f'Box Plot: {col}', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')

            # Plot 3: Statistiques textuelles
            ax = axes[idx, 2]
            ax.axis('off')

            stats_text = f"""
STATISTIQUES COMPARATIVES

R√©f√©rence (Entra√Ænement):
  Mean     : {ref_stats['mean']:>12,.2f}
  Median   : {ref_stats['median']:>12,.2f}
  Std      : {ref_stats['std']:>12,.2f}
  Q25-Q75  : {ref_stats['q25']:>12,.2f} - {ref_stats['q75']:,.2f}
  Zeros    : {ref_stats['zeros']:>12.1%}

Nouvelles Donn√©es:
  Mean     : {new_stats['mean']:>12,.2f}
  Median   : {new_stats['median']:>12,.2f}
  Std      : {new_stats['std']:>12,.2f}
  Q25-Q75  : {new_stats['q25']:>12,.2f} - {new_stats['q75']:,.2f}
  Zeros    : {new_stats['zeros']:>12.1%}

√âCARTS:
  Mean     : {mean_diff_pct:>12.1f}%
  Median   : {median_diff_pct:>12.1f}%
"""

            if abs(mean_diff_pct) > 20 or abs(median_diff_pct) > 20:
                stats_text += "\nüö® ALERTE: Shift significatif!"

            ax.text(0.1, 0.5, stats_text, transform=ax.transAxes,
                   fontsize=11, verticalalignment='center', family='monospace',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()
        output_path = self.output_dir / '01_numeric_distributions.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"\n‚úÖ Graphique sauvegard√©: {output_path}")
        plt.close()

        return pd.DataFrame(results)

    def analyze_categorical_distributions(self):
        """Analyser les distributions cat√©gorielles"""
        print("\n" + "="*80)
        print("üìä ANALYSE DES DISTRIBUTIONS CAT√âGORIELLES")
        print("="*80)

        results = []

        for col in self.categorical_cols:
            if col not in self.df_ref.columns or col not in self.df_new.columns:
                continue

            print(f"\nüìä {col}:")

            # Distributions
            ref_dist = self.df_ref[col].value_counts(normalize=True)
            new_dist = self.df_new[col].value_counts(normalize=True)

            # Valeurs communes et nouvelles
            ref_values = set(ref_dist.index)
            new_values = set(new_dist.index)

            only_in_ref = ref_values - new_values
            only_in_new = new_values - ref_values
            common_values = ref_values & new_values

            print(f"   Valeurs dans R√©f√©rence: {len(ref_values)}")
            print(f"   Valeurs dans Nouvelles: {len(new_values)}")
            print(f"   Valeurs communes: {len(common_values)}")

            if only_in_new:
                print(f"   üö® NOUVELLES VALEURS (absentes du train): {len(only_in_new)}")
                if len(only_in_new) <= 10:
                    for val in list(only_in_new)[:10]:
                        count = (self.df_new[col] == val).sum()
                        pct = count / len(self.df_new) * 100
                        print(f"      - {val}: {count} cas ({pct:.1f}%)")

            # Top 10 valeurs - comparaison
            print(f"\n   Top 10 valeurs - Comparaison:")
            top_ref = ref_dist.head(10)
            for val, ref_pct in top_ref.items():
                new_pct = new_dist.get(val, 0)
                diff_pct = (new_pct - ref_pct) * 100
                print(f"      {val[:40]:40s}: Ref {ref_pct:5.1%} | New {new_pct:5.1%} | Diff {diff_pct:+5.1f}pp")

            results.append({
                'Variable': col,
                'Ref_Unique': len(ref_values),
                'New_Unique': len(new_values),
                'Common': len(common_values),
                'Only_in_New': len(only_in_new),
                'Only_in_Ref': len(only_in_ref),
                'Coverage_%': len(common_values) / len(new_values) * 100 if len(new_values) > 0 else 0
            })

        return pd.DataFrame(results)

    def analyze_engineered_features(self):
        """Analyser les features engineered (taux de fond√©e, etc.)"""
        print("\n" + "="*80)
        print("üìä ANALYSE DES FEATURES ENGINEERED")
        print("="*80)

        # Cr√©er preprocessor et fit sur r√©f√©rence
        preprocessor = ProductionPreprocessorV2(min_samples_stats=30)

        if 'Fondee' not in self.df_ref.columns:
            print("‚ö†Ô∏è  Colonne 'Fondee' manquante dans donn√©es de r√©f√©rence")
            print("   Impossible d'analyser les taux de fond√©e")
            return None

        preprocessor.fit(self.df_ref)

        # Analyser la couverture des taux de fond√©e
        print("\nüìä Couverture des statistiques robustes (min 30 cas sur train):")

        results = []

        # Familles
        if 'Famille Produit' in self.df_new.columns:
            new_families = self.df_new['Famille Produit'].unique()
            families_with_stats = set(preprocessor.family_stats['taux'].keys())
            coverage = sum([f in families_with_stats for f in new_families]) / len(new_families) * 100

            missing_families = [f for f in new_families if f not in families_with_stats]
            missing_count = sum([self.df_new['Famille Produit'] == f for f in missing_families]).sum()

            print(f"\n   Famille Produit:")
            print(f"      Familles dans nouvelles donn√©es: {len(new_families)}")
            print(f"      Familles avec stats robustes (train): {len(families_with_stats)}")
            print(f"      Couverture: {coverage:.1f}%")
            print(f"      üö® Familles sans stats: {len(missing_families)} ({missing_count} cas)")

            if missing_families and len(missing_families) <= 20:
                print(f"      Familles manquantes:")
                for fam in missing_families[:20]:
                    count = (self.df_new['Famille Produit'] == fam).sum()
                    pct = count / len(self.df_new) * 100
                    print(f"         - {fam}: {count} cas ({pct:.1f}%)")

            results.append({
                'Feature': 'taux_fondee_famille',
                'Total_New_Values': len(new_families),
                'With_Stats': len([f for f in new_families if f in families_with_stats]),
                'Coverage_%': coverage,
                'Missing_Cases': missing_count,
                'Missing_%': missing_count / len(self.df_new) * 100
            })

        # Cat√©gories
        if 'Cat√©gorie' in self.df_new.columns:
            new_cats = self.df_new['Cat√©gorie'].unique()
            cats_with_stats = set(preprocessor.category_stats['taux'].keys())
            coverage = sum([c in cats_with_stats for c in new_cats]) / len(new_cats) * 100
            missing_count = sum([self.df_new['Cat√©gorie'] == c for c in new_cats if c not in cats_with_stats]).sum()

            print(f"\n   Cat√©gorie:")
            print(f"      Couverture: {coverage:.1f}%")
            print(f"      üö® Cas sans stats: {missing_count} ({missing_count/len(self.df_new)*100:.1f}%)")

            results.append({
                'Feature': 'taux_fondee_categorie',
                'Total_New_Values': len(new_cats),
                'With_Stats': len([c for c in new_cats if c in cats_with_stats]),
                'Coverage_%': coverage,
                'Missing_Cases': missing_count,
                'Missing_%': missing_count / len(self.df_new) * 100
            })

        # Sous-cat√©gories
        if 'Sous-cat√©gorie' in self.df_new.columns:
            new_subcats = self.df_new['Sous-cat√©gorie'].unique()
            subcats_with_stats = set(preprocessor.subcategory_stats['taux'].keys())
            coverage = sum([s in subcats_with_stats for s in new_subcats]) / len(new_subcats) * 100
            missing_count = sum([self.df_new['Sous-cat√©gorie'] == s for s in new_subcats if s not in subcats_with_stats]).sum()

            print(f"\n   Sous-cat√©gorie:")
            print(f"      Couverture: {coverage:.1f}%")
            print(f"      üö® Cas sans stats: {missing_count} ({missing_count/len(self.df_new)*100:.1f}%)")

            results.append({
                'Feature': 'taux_fondee_souscategorie',
                'Total_New_Values': len(new_subcats),
                'With_Stats': len([s for s in new_subcats if s in subcats_with_stats]),
                'Coverage_%': coverage,
                'Missing_Cases': missing_count,
                'Missing_%': missing_count / len(self.df_new) * 100
            })

        # Segments
        if 'Segment' in self.df_new.columns:
            new_segs = self.df_new['Segment'].unique()
            segs_with_stats = set(preprocessor.segment_stats['taux'].keys())
            coverage = sum([s in segs_with_stats for s in new_segs]) / len(new_segs) * 100
            missing_count = sum([self.df_new['Segment'] == s for s in new_segs if s not in segs_with_stats]).sum()

            print(f"\n   Segment:")
            print(f"      Couverture: {coverage:.1f}%")
            print(f"      üö® Cas sans stats: {missing_count} ({missing_count/len(self.df_new)*100:.1f}%)")

            results.append({
                'Feature': 'taux_fondee_segment',
                'Total_New_Values': len(new_segs),
                'With_Stats': len([s for s in new_segs if s in segs_with_stats]),
                'Coverage_%': coverage,
                'Missing_Cases': missing_count,
                'Missing_%': missing_count / len(self.df_new) * 100
            })

        return pd.DataFrame(results)

    def generate_summary_report(self, numeric_results, categorical_results, engineered_results):
        """G√©n√©rer rapport r√©capitulatif"""
        print("\n" + "="*80)
        print("üìÑ G√âN√âRATION DU RAPPORT DE DIAGNOSTIC")
        print("="*80)

        report_path = self.output_dir / f'rapport_diagnostic_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("RAPPORT DE DIAGNOSTIC - DISTRIBUTION SHIFT\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")

            f.write(f"Donn√©es de r√©f√©rence (train): {self.reference_file}\n")
            f.write(f"Nouvelles donn√©es (inf√©rence): {self.new_file}\n\n")

            f.write(f"Nombre de lignes:\n")
            f.write(f"  R√©f√©rence: {len(self.df_ref)}\n")
            f.write(f"  Nouvelles: {len(self.df_new)}\n\n")

            # R√©sum√© num√©riques
            f.write("="*80 + "\n")
            f.write("1. VARIABLES NUM√âRIQUES - SHIFT D√âTECT√â\n")
            f.write("="*80 + "\n\n")

            if numeric_results is not None and len(numeric_results) > 0:
                # Identifier les shifts importants
                major_shifts = numeric_results[
                    (abs(numeric_results['Mean_Diff_%']) > 20) |
                    (abs(numeric_results['Median_Diff_%']) > 20)
                ]

                if len(major_shifts) > 0:
                    f.write("üö® ALERTES: Variables avec shift > 20%:\n\n")
                    for _, row in major_shifts.iterrows():
                        f.write(f"  {row['Variable']}:\n")
                        f.write(f"    Mean shift: {row['Mean_Diff_%']:+.1f}%\n")
                        f.write(f"    Median shift: {row['Median_Diff_%']:+.1f}%\n")
                        f.write(f"    R√©f√©rence Mean: {row['Ref_Mean']:,.2f}\n")
                        f.write(f"    Nouvelles Mean: {row['New_Mean']:,.2f}\n\n")
                else:
                    f.write("‚úÖ Pas de shift majeur d√©tect√© (< 20%)\n\n")

                f.write("\nD√©tails complets:\n")
                f.write(numeric_results.to_string())
                f.write("\n\n")

            # R√©sum√© cat√©gorielles
            f.write("="*80 + "\n")
            f.write("2. VARIABLES CAT√âGORIELLES - NOUVELLES VALEURS\n")
            f.write("="*80 + "\n\n")

            if categorical_results is not None and len(categorical_results) > 0:
                # Identifier les probl√®mes
                low_coverage = categorical_results[categorical_results['Coverage_%'] < 80]

                if len(low_coverage) > 0:
                    f.write("üö® ALERTES: Variables avec faible couverture (< 80%):\n\n")
                    for _, row in low_coverage.iterrows():
                        f.write(f"  {row['Variable']}:\n")
                        f.write(f"    Couverture: {row['Coverage_%']:.1f}%\n")
                        f.write(f"    Nouvelles valeurs absentes du train: {row['Only_in_New']}\n\n")
                else:
                    f.write("‚úÖ Bonne couverture cat√©gorielle (> 80%)\n\n")

                f.write("\nD√©tails complets:\n")
                f.write(categorical_results.to_string())
                f.write("\n\n")

            # R√©sum√© features engineered
            f.write("="*80 + "\n")
            f.write("3. FEATURES ENGINEERED - TAUX DE FOND√âE\n")
            f.write("="*80 + "\n\n")

            if engineered_results is not None and len(engineered_results) > 0:
                # Identifier les probl√®mes
                low_coverage = engineered_results[engineered_results['Coverage_%'] < 80]

                if len(low_coverage) > 0:
                    f.write("üö® ALERTES: Features avec stats manquantes:\n\n")
                    for _, row in low_coverage.iterrows():
                        f.write(f"  {row['Feature']}:\n")
                        f.write(f"    Couverture: {row['Coverage_%']:.1f}%\n")
                        f.write(f"    Cas sans stats: {row['Missing_Cases']} ({row['Missing_%']:.1f}%)\n")
                        f.write(f"    ‚Üí Ces cas utilisent le taux global (fallback)\n\n")
                else:
                    f.write("‚úÖ Bonne couverture des stats (> 80%)\n\n")

                f.write("\nD√©tails complets:\n")
                f.write(engineered_results.to_string())
                f.write("\n\n")

            # Recommandations
            f.write("="*80 + "\n")
            f.write("4. RECOMMANDATIONS\n")
            f.write("="*80 + "\n\n")

            recommendations = []

            # Bas√© sur les r√©sultats
            if numeric_results is not None and len(numeric_results) > 0:
                major_shifts = numeric_results[
                    (abs(numeric_results['Mean_Diff_%']) > 20) |
                    (abs(numeric_results['Median_Diff_%']) > 20)
                ]
                if len(major_shifts) > 0:
                    recommendations.append(
                        f"1. SHIFT NUM√âRIQUE MAJEUR d√©tect√© sur {len(major_shifts)} variable(s):\n"
                        f"   - Les distributions de {', '.join(major_shifts['Variable'].tolist())} ont chang√© significativement\n"
                        f"   - Le mod√®le a √©t√© entra√Æn√© sur des donn√©es avec des distributions diff√©rentes\n"
                        f"   - Solution: R√©-entra√Æner le mod√®le en incluant les donn√©es 2023 dans le train\n"
                    )

            if categorical_results is not None and len(categorical_results) > 0:
                new_values_total = categorical_results['Only_in_New'].sum()
                if new_values_total > 0:
                    recommendations.append(
                        f"2. NOUVELLES VALEURS CAT√âGORIELLES: {new_values_total} nouvelles valeurs au total\n"
                        f"   - Ces valeurs n'existaient pas dans les donn√©es d'entra√Ænement\n"
                        f"   - Le mod√®le les traite avec des fr√©quences = 0\n"
                        f"   - Solution: R√©-entra√Æner en incluant 2023 pour capturer ces nouvelles valeurs\n"
                    )

            if engineered_results is not None and len(engineered_results) > 0:
                missing_stats_total = engineered_results['Missing_Cases'].sum()
                if missing_stats_total > 100:
                    recommendations.append(
                        f"3. TAUX DE FOND√âE MANQUANTS: {missing_stats_total} cas utilisent le fallback\n"
                        f"   - Ces cas n'ont pas de taux de fond√©e sp√©cifique (< 30 √©chantillons dans train)\n"
                        f"   - Ils utilisent le taux global, moins pr√©cis\n"
                        f"   - Impact: Pr√©dictions moins fiables pour ces cas\n"
                        f"   - Solution: R√©-entra√Æner avec 2023 pour enrichir les statistiques\n"
                    )

            if not recommendations:
                recommendations.append("‚úÖ Pas de probl√®me majeur d√©tect√©. Les distributions sont similaires.")

            for rec in recommendations:
                f.write(rec + "\n")

            f.write("\n" + "="*80 + "\n")
            f.write("CONCLUSION:\n")
            f.write("="*80 + "\n\n")

            if recommendations and len(recommendations) > 1:
                f.write("Les donn√©es de 2023 sont SIGNIFICATIVEMENT DIFF√âRENTES des donn√©es 2024/2025.\n")
                f.write("Cela explique les probabilit√©s faibles observ√©es lors de l'inf√©rence.\n\n")
                f.write("SOLUTION RECOMMAND√âE:\n")
                f.write("  1. R√©-entra√Æner le mod√®le en incluant 2023 dans les donn√©es d'entra√Ænement\n")
                f.write("  2. Ou utiliser uniquement 2023 comme train si vous pr√©disez sur 2023\n")
                f.write("  3. Recalculer les taux de fond√©e sur la p√©riode appropri√©e (2023 ou 2023+2024)\n\n")
            else:
                f.write("Les distributions semblent similaires. Le probl√®me peut venir d'ailleurs.\n\n")

        print(f"‚úÖ Rapport sauvegard√©: {report_path}")

        return report_path

    def run(self):
        """Ex√©cuter le diagnostic complet"""
        self.load_data()

        numeric_results = self.analyze_numeric_distributions()
        categorical_results = self.analyze_categorical_distributions()
        engineered_results = self.analyze_engineered_features()

        report_path = self.generate_summary_report(numeric_results, categorical_results, engineered_results)

        print("\n" + "="*80)
        print("‚úÖ DIAGNOSTIC TERMIN√â")
        print("="*80)
        print(f"\nüìÇ R√©sultats dans: {self.output_dir}")
        print(f"üìÑ Rapport: {report_path}")
        print(f"üìä Graphiques: 01_numeric_distributions.png")


def main():
    parser = argparse.ArgumentParser(description='Diagnostic de distribution shift')
    parser.add_argument('--reference_file', type=str, required=True,
                       help='Fichier de r√©f√©rence (donn√©es d\'entra√Ænement, ex: 2024)')
    parser.add_argument('--new_file', type=str, required=True,
                       help='Nouvelles donn√©es (inf√©rence, ex: 2023)')

    args = parser.parse_args()

    diagnostic = DistributionShiftDiagnostic(args.reference_file, args.new_file)
    diagnostic.run()


if __name__ == '__main__':
    main()
